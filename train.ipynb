{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26134228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#全局变量\n",
    "hub_token =None # open('/root/hub_token.txt').read().strip()\n",
    "repo_id = 'BirdL/DALL-E-Cats'#'BirdL/DALL-E-Dogs'#'BirdL/DALL-E-Cats'#'lansinuote/diffusion.1.unconditional'ULZIITOGTOKH/cat_images\n",
    "push_to_hub =False#True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8560ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    #加载数据集\n",
    "    dataset = load_dataset(repo_id)#('BirdL/DALL-E-Cats')#('huggan/flowers-102-categories', download=True)\n",
    "    #dataset = load_dataset('huggan/pokemon') \n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9279d265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['Images'],\n",
       "     num_rows: 300\n",
       " }),\n",
       " {'Images': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=256x256>})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(path=repo_id, split='train')\n",
    "\n",
    "\n",
    "dataset, dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad3de17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Images': Image(decode=True, id=None)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(path=repo_id, split='train')\n",
    "\n",
    "# 查看数据集的特征信息\n",
    "print(dataset.features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "102e0a89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['Images'],\n",
       "     num_rows: 300\n",
       " }),\n",
       " {'Images': tensor([[[-0.6471, -0.6471, -0.6392,  ..., -0.6941, -0.6941, -0.7020],\n",
       "           [-0.6314, -0.6392, -0.6314,  ..., -0.6941, -0.6941, -0.6941],\n",
       "           [-0.6157, -0.6314, -0.6235,  ..., -0.6863, -0.6863, -0.6941],\n",
       "           ...,\n",
       "           [-0.8118, -0.8118, -0.8039,  ..., -0.7647, -0.7725, -0.7725],\n",
       "           [-0.8118, -0.8118, -0.8118,  ..., -0.7725, -0.7647, -0.7647],\n",
       "           [-0.8275, -0.8353, -0.8196,  ..., -0.7804, -0.7804, -0.7725]],\n",
       "  \n",
       "          [[-0.6863, -0.6784, -0.6784,  ..., -0.7176, -0.7255, -0.7176],\n",
       "           [-0.6627, -0.6627, -0.6706,  ..., -0.7176, -0.7098, -0.7020],\n",
       "           [-0.6471, -0.6549, -0.6549,  ..., -0.7176, -0.7098, -0.7098],\n",
       "           ...,\n",
       "           [-0.8196, -0.8118, -0.8196,  ..., -0.7804, -0.7804, -0.7804],\n",
       "           [-0.8196, -0.8196, -0.8196,  ..., -0.7804, -0.7804, -0.7804],\n",
       "           [-0.8431, -0.8275, -0.8275,  ..., -0.7961, -0.7882, -0.7804]],\n",
       "  \n",
       "          [[-0.6941, -0.6784, -0.6863,  ..., -0.7176, -0.7176, -0.7098],\n",
       "           [-0.6706, -0.6706, -0.6627,  ..., -0.7098, -0.7020, -0.6941],\n",
       "           [-0.6549, -0.6549, -0.6471,  ..., -0.7020, -0.6941, -0.6863],\n",
       "           ...,\n",
       "           [-0.8275, -0.8196, -0.8196,  ..., -0.7725, -0.7725, -0.7725],\n",
       "           [-0.8353, -0.8275, -0.8275,  ..., -0.7804, -0.7804, -0.7804],\n",
       "           [-0.8431, -0.8353, -0.8275,  ..., -0.7882, -0.7882, -0.7804]]])})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "#图像增强\n",
    "compose = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(\n",
    "       128, interpolation=torchvision.transforms.InterpolationMode.BILINEAR),#64  128  32\n",
    "    torchvision.transforms.RandomCrop(128),\n",
    "    #torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    image = [compose(i) for i in data['Images']]\n",
    "    return {'Images': image}# return {'image': image}\n",
    "\n",
    "\n",
    "#因为图像增强在每个epoch中要动态计算,所以不能简单地用map处理\n",
    "dataset.set_transform(f)\n",
    "\n",
    "dataset, dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ef5b1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38,\n",
       " {'Images': tensor([[[[-1.0000, -1.0000, -0.9922,  ..., -0.9765, -0.9765, -0.9765],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.9843, -0.9765, -0.9765],\n",
       "            [-1.0000, -0.9922, -0.9922,  ..., -0.9843, -0.9765, -0.9765],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ...,  0.0039, -0.0196, -0.0353],\n",
       "            [-1.0000, -1.0000, -1.0000,  ...,  0.0275, -0.0196, -0.0275],\n",
       "            [-1.0000, -1.0000, -1.0000,  ...,  0.0039, -0.0353, -0.0039]],\n",
       "  \n",
       "           [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.0353, -0.0588, -0.0745],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.0196, -0.0745, -0.0980],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.0588, -0.0824, -0.0588]],\n",
       "  \n",
       "           [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -0.9922],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -0.9922],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -0.9922],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.0667, -0.0824, -0.0980],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.0510, -0.1059, -0.1216],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.0902, -0.1216, -0.0745]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.0000, -1.0000, -1.0000,  ..., -0.9608, -0.9529, -0.9529],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.9608, -0.9529, -0.9608],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.9608, -0.9529, -0.9608],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.9608, -0.9608, -0.9686],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.9608, -0.9608, -0.9529],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.9608, -0.9608, -0.9529]],\n",
       "  \n",
       "           [[-1.0000, -1.0000, -1.0000,  ..., -0.9765, -0.9686, -0.9608],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.9765, -0.9686, -0.9608],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.9765, -0.9765, -0.9608],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.9686, -0.9686, -0.9686],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.9686, -0.9686, -0.9608],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.9765, -0.9686, -0.9529]],\n",
       "  \n",
       "           [[-1.0000, -1.0000, -1.0000,  ..., -0.9451, -0.9373, -0.9137],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.9373, -0.9294, -0.9137],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.9373, -0.9216, -0.9137],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.9373, -0.9294, -0.9216],\n",
       "            [-0.9922, -1.0000, -0.9922,  ..., -0.9451, -0.9373, -0.9294],\n",
       "            [-0.9843, -0.9922, -0.9922,  ..., -0.9373, -0.9373, -0.9216]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            ...,\n",
       "            [-0.7961, -0.7882, -0.7882,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-0.7882, -0.7882, -0.7882,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-0.7882, -0.7961, -0.7961,  ..., -1.0000, -1.0000, -1.0000]],\n",
       "  \n",
       "           [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            ...,\n",
       "            [-0.7882, -0.7804, -0.7804,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-0.7804, -0.7882, -0.7882,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-0.7961, -0.7882, -0.7882,  ..., -1.0000, -1.0000, -1.0000]],\n",
       "  \n",
       "           [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -0.9922],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -0.9922],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            ...,\n",
       "            [-0.8353, -0.8196, -0.8118,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-0.8275, -0.8275, -0.8275,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-0.8353, -0.8353, -0.8275,  ..., -1.0000, -1.0000, -1.0000]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[-0.9686, -0.9608, -0.9608,  ..., -0.9608, -0.9686, -0.9686],\n",
       "            [-0.9608, -0.9608, -0.9608,  ..., -0.9686, -0.9686, -0.9686],\n",
       "            [-0.9529, -0.9529, -0.9529,  ..., -0.9765, -0.9686, -0.9765],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.8510, -0.8667, -0.8745],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.8510, -0.8588, -0.8745],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.8745, -0.8745, -0.8824]],\n",
       "  \n",
       "           [[-0.9765, -0.9765, -0.9765,  ..., -0.9922, -0.9922, -0.9765],\n",
       "            [-0.9686, -0.9765, -0.9765,  ..., -0.9922, -0.9843, -0.9765],\n",
       "            [-0.9765, -0.9686, -0.9765,  ..., -0.9922, -0.9922, -0.9765],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.8431, -0.8510, -0.8667],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.8510, -0.8588, -0.8667],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.8667, -0.8745, -0.8745]],\n",
       "  \n",
       "           [[-0.9843, -0.9765, -0.9843,  ..., -1.0000, -1.0000, -0.9922],\n",
       "            [-0.9686, -0.9765, -0.9686,  ..., -1.0000, -0.9922, -0.9843],\n",
       "            [-0.9686, -0.9608, -0.9686,  ..., -1.0000, -0.9843, -0.9922],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.8431, -0.8431, -0.8510],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.8510, -0.8588, -0.8667],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.8510, -0.8667, -0.8745]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.9922, -0.9922, -0.9922,  ..., -0.9451, -0.9529, -0.9529],\n",
       "            [-0.9922, -0.9922, -0.9922,  ..., -0.9529, -0.9451, -0.9529],\n",
       "            [-0.9843, -0.9922, -0.9843,  ..., -0.9529, -0.9451, -0.9529],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
       "  \n",
       "           [[-1.0000, -1.0000, -1.0000,  ..., -0.9922, -0.9922, -0.9843],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.9922, -0.9922, -0.9843],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.9922, -0.9922, -0.9843],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
       "  \n",
       "           [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -0.9922],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -0.9922, -0.9922],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -0.9922, -0.9922],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.7098, -0.7098, -0.7255,  ..., -0.4353, -0.4196, -0.4196],\n",
       "            [-0.7020, -0.7176, -0.7176,  ..., -0.4196, -0.4039, -0.4039],\n",
       "            [-0.7020, -0.7098, -0.7098,  ..., -0.4039, -0.4039, -0.4039],\n",
       "            ...,\n",
       "            [-0.6078, -0.5765, -0.4353,  ..., -0.4039, -0.4588, -0.6471],\n",
       "            [-0.5373, -0.4824, -0.3961,  ..., -0.3098, -0.2941, -0.4039],\n",
       "            [-0.9216, -0.8745, -0.7961,  ..., -0.6235, -0.6235, -0.7804]],\n",
       "  \n",
       "           [[-0.9216, -0.9137, -0.9294,  ..., -0.2314, -0.2314, -0.2078],\n",
       "            [-0.8980, -0.9059, -0.9137,  ..., -0.2235, -0.2157, -0.2000],\n",
       "            [-0.8980, -0.8902, -0.8980,  ..., -0.2078, -0.2078, -0.1922],\n",
       "            ...,\n",
       "            [-0.5529, -0.5608, -0.4275,  ..., -0.3569, -0.4039, -0.6157],\n",
       "            [-0.4588, -0.4588, -0.3882,  ..., -0.2627, -0.2392, -0.3804],\n",
       "            [-0.9059, -0.8667, -0.8118,  ..., -0.5843, -0.5686, -0.7412]],\n",
       "  \n",
       "           [[-1.0000, -1.0000, -1.0000,  ..., -0.2863, -0.2627, -0.2471],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.2627, -0.2392, -0.2235],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -0.2471, -0.2314, -0.2157],\n",
       "            ...,\n",
       "            [-0.6549, -0.6078, -0.4510,  ..., -0.4039, -0.4745, -0.6941],\n",
       "            [-0.5137, -0.4902, -0.4275,  ..., -0.3412, -0.3412, -0.4431],\n",
       "            [-0.9059, -0.8745, -0.8824,  ..., -0.6941, -0.6941, -0.7882]]]])})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=8,#16  16（50，1280128）32（50 128 128）\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=False)\n",
    "\n",
    "len(loader), next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9026717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14998.4003"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "#定义模型,随机初始化参数\n",
    "model = UNet2DModel(\n",
    "    sample_size=64,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=3,#初始是2\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        'DownBlock2D',\n",
    "        'DownBlock2D',\n",
    "        'DownBlock2D',\n",
    "        'DownBlock2D',\n",
    "        'AttnDownBlock2D',\n",
    "        'DownBlock2D',\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        'UpBlock2D',\n",
    "        'AttnUpBlock2D',\n",
    "        'UpBlock2D',\n",
    "        'UpBlock2D',\n",
    "        'UpBlock2D',\n",
    "        'UpBlock2D',\n",
    "    ),\n",
    ")\n",
    "\n",
    "sum(i.numel() for i in model.parameters()) / 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2969101e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DDPMScheduler {\n",
       "   \"_class_name\": \"DDPMScheduler\",\n",
       "   \"_diffusers_version\": \"0.27.2\",\n",
       "   \"beta_end\": 0.02,\n",
       "   \"beta_schedule\": \"linear\",\n",
       "   \"beta_start\": 0.0001,\n",
       "   \"clip_sample\": true,\n",
       "   \"clip_sample_range\": 1.0,\n",
       "   \"dynamic_thresholding_ratio\": 0.995,\n",
       "   \"num_train_timesteps\": 1000,\n",
       "   \"prediction_type\": \"epsilon\",\n",
       "   \"rescale_betas_zero_snr\": false,\n",
       "   \"sample_max_value\": 1.0,\n",
       "   \"steps_offset\": 0,\n",
       "   \"thresholding\": false,\n",
       "   \"timestep_spacing\": \"leading\",\n",
       "   \"trained_betas\": null,\n",
       "   \"variance_type\": \"fixed_small\"\n",
       " },\n",
       " AdamW (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.995, 0.999)\n",
       "     capturable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     initial_lr: 1e-05\n",
       "     lr: 0.0\n",
       "     maximize: False\n",
       "     weight_decay: 1e-06\n",
       " ),\n",
       " <torch.optim.lr_scheduler.LambdaLR at 0x1f0cc019370>,\n",
       " MSELoss())"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "from diffusers.optimization import get_scheduler\n",
    "\n",
    "#初始化工具类\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000,\n",
    "                          beta_schedule='linear',\n",
    "                          prediction_type='epsilon')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=1e-5,#1e-4 1e-5\n",
    "                              betas=(0.995, 0.999),#0.95\n",
    "                              weight_decay=1e-6,\n",
    "                              eps=1e-8)\n",
    "\n",
    "scheduler_lr = get_scheduler('cosine',\n",
    "                             optimizer=optimizer,\n",
    "                             num_warmup_steps=500,\n",
    "                             num_training_steps=len(loader) * 100)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "scheduler, optimizer, scheduler_lr, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22436df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1282, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_loss(image):\n",
    "    device = image.device\n",
    "    \n",
    "    #随机噪声\n",
    "    #[b, 3, 64, 64]\n",
    "    noise = torch.randn(image.shape).to(device)\n",
    "\n",
    "    #随机b个噪声步数\n",
    "    #1000 = scheduler.config.num_train_timesteps\n",
    "    #[b]\n",
    "    noise_step = torch.randint(0, 1000, (image.shape[0], ),\n",
    "                               device=device).long()\n",
    "\n",
    "    #往图片当中添加噪声\n",
    "    #[b, 3, 64, 64]\n",
    "    image_noise = scheduler.add_noise(image, noise, noise_step)\n",
    "\n",
    "    #把图片里的噪声计算出来\n",
    "    #[b, 3, 64, 64]\n",
    "    out = model(image_noise, noise_step).sample\n",
    "\n",
    "    #求mse loss\n",
    "    return criterion(out, noise)\n",
    "\n",
    "\n",
    "get_loss(torch.randn(16, 3, 128,128))#6464"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17e0114e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.013173873205424139\n",
      "Epoch 1, Loss: 0.015470477176438036\n",
      "Epoch 2, Loss: 0.011140411765633249\n",
      "Epoch 3, Loss: 0.014147320259852629\n",
      "Epoch 4, Loss: 0.01410327803368043\n",
      "Epoch 5, Loss: 0.01451598870641503\n",
      "Epoch 6, Loss: 0.013111798374570514\n",
      "Epoch 7, Loss: 0.011961815131239985\n",
      "Epoch 8, Loss: 0.012987377796028005\n",
      "Epoch 9, Loss: 0.010991242983819623\n",
      "Epoch 10, Loss: 0.015370951821463868\n",
      "Epoch 11, Loss: 0.017736459261198576\n",
      "Epoch 12, Loss: 0.010049318671103959\n",
      "Epoch 13, Loss: 0.012128549814836955\n",
      "Epoch 14, Loss: 0.016689623283271334\n",
      "Epoch 15, Loss: 0.014736769344093964\n",
      "Epoch 16, Loss: 0.01350845473720447\n",
      "Epoch 17, Loss: 0.012170238702214863\n",
      "Epoch 18, Loss: 0.012282445358945742\n",
      "Epoch 19, Loss: 0.015477474999467009\n",
      "Epoch 20, Loss: 0.011508478145850333\n",
      "Epoch 21, Loss: 0.010854914585299986\n",
      "Epoch 22, Loss: 0.013542265514843166\n",
      "Epoch 23, Loss: 0.012367168986728709\n",
      "Epoch 24, Loss: 0.011881040825851653\n",
      "Epoch 25, Loss: 0.017416678191358715\n",
      "Epoch 26, Loss: 0.009683870471474764\n",
      "Epoch 27, Loss: 0.014437914614590179\n",
      "Epoch 28, Loss: 0.013291634214845928\n",
      "Epoch 29, Loss: 0.01241614529863\n",
      "Epoch 30, Loss: 0.008817826992047853\n",
      "Epoch 31, Loss: 0.014765966965473797\n",
      "Epoch 32, Loss: 0.011924500711948463\n",
      "Epoch 33, Loss: 0.011506912617110893\n",
      "Epoch 34, Loss: 0.014505663047250556\n",
      "Epoch 35, Loss: 0.015144003343792926\n",
      "Epoch 36, Loss: 0.014106520041088132\n",
      "Epoch 37, Loss: 0.009898704930061572\n",
      "Epoch 38, Loss: 0.014459688001998552\n",
      "Epoch 39, Loss: 0.013410479421604816\n",
      "Epoch 40, Loss: 0.013784175778518579\n",
      "Epoch 41, Loss: 0.011647103706644358\n",
      "Epoch 42, Loss: 0.013892470129863605\n",
      "Epoch 43, Loss: 0.011178406090230533\n",
      "Epoch 44, Loss: 0.011206239369991971\n",
      "Epoch 45, Loss: 0.012983449498853205\n",
      "Epoch 46, Loss: 0.013669997799871979\n",
      "Epoch 47, Loss: 0.010332646117986817\n",
      "Epoch 48, Loss: 0.011580140400359309\n",
      "Epoch 49, Loss: 0.0129412854041316\n",
      "Epoch 50, Loss: 0.012702092179097235\n",
      "Epoch 51, Loss: 0.012089615595821095\n",
      "Epoch 52, Loss: 0.015243238230284891\n",
      "Epoch 53, Loss: 0.013595261948036128\n",
      "Epoch 54, Loss: 0.010973562903114055\n",
      "Epoch 55, Loss: 0.010487086593026393\n",
      "Epoch 56, Loss: 0.01023352119189344\n",
      "Epoch 57, Loss: 0.01272103749828315\n",
      "Epoch 58, Loss: 0.013650762358386265\n",
      "Epoch 59, Loss: 0.01516168296802789\n",
      "Epoch 60, Loss: 0.012155100482663042\n",
      "Epoch 61, Loss: 0.013684984149509355\n",
      "Epoch 62, Loss: 0.010954319762899295\n",
      "Epoch 63, Loss: 0.011952555707753882\n",
      "Epoch 64, Loss: 0.014311320185171146\n",
      "Epoch 65, Loss: 0.01195037223423194\n",
      "Epoch 66, Loss: 0.01339694977076234\n",
      "Epoch 67, Loss: 0.01403206106471388\n",
      "Epoch 68, Loss: 0.012836527955522271\n",
      "Epoch 69, Loss: 0.01326325026610376\n",
      "Epoch 70, Loss: 0.013162059145734498\n",
      "Epoch 71, Loss: 0.013594384911764217\n",
      "Epoch 72, Loss: 0.013901796087769694\n",
      "Epoch 73, Loss: 0.013104853410224774\n",
      "Epoch 74, Loss: 0.015576568701745648\n",
      "Epoch 75, Loss: 0.010467523531253008\n",
      "Epoch 76, Loss: 0.013342280369742136\n",
      "Epoch 77, Loss: 0.010785460582395134\n",
      "Epoch 78, Loss: 0.009991461922733211\n",
      "Epoch 79, Loss: 0.012425804498458379\n",
      "Epoch 80, Loss: 0.013175659531139229\n",
      "Epoch 81, Loss: 0.010802196669637373\n",
      "Epoch 82, Loss: 0.012372809266181369\n",
      "Epoch 83, Loss: 0.011897810930876355\n",
      "Epoch 84, Loss: 0.013370202143529528\n",
      "Epoch 85, Loss: 0.012228608394874945\n",
      "Epoch 86, Loss: 0.011472703435605294\n",
      "Epoch 87, Loss: 0.013100028571084534\n",
      "Epoch 88, Loss: 0.011814565066934415\n",
      "Epoch 89, Loss: 0.013448919580121966\n",
      "Epoch 90, Loss: 0.012575514988336516\n",
      "Epoch 91, Loss: 0.01442624570948905\n",
      "Epoch 92, Loss: 0.011673041748030013\n",
      "Epoch 93, Loss: 0.012417802850617781\n",
      "Epoch 94, Loss: 0.011848352068275409\n",
      "Epoch 95, Loss: 0.010475078573156344\n",
      "Epoch 96, Loss: 0.012172014452517033\n",
      "Epoch 97, Loss: 0.00982834800685707\n",
      "Epoch 98, Loss: 0.010073334265095917\n",
      "Epoch 99, Loss: 0.012258490945800747\n",
      "Epoch 100, Loss: 0.013893227001014901\n",
      "Epoch 101, Loss: 0.01097270433398846\n",
      "Epoch 102, Loss: 0.015622978693021363\n",
      "Epoch 103, Loss: 0.010568948480986842\n",
      "Epoch 104, Loss: 0.012726918659418038\n",
      "Epoch 105, Loss: 0.00956632873950232\n",
      "Epoch 106, Loss: 0.016075210964405222\n",
      "Epoch 107, Loss: 0.011157911635420629\n",
      "Epoch 108, Loss: 0.012725859065540135\n",
      "Epoch 109, Loss: 0.010503967776649484\n",
      "Epoch 110, Loss: 0.010691798347244529\n",
      "Epoch 111, Loss: 0.01585882649707951\n",
      "Epoch 112, Loss: 0.011159681476113436\n",
      "Epoch 113, Loss: 0.01585026624563493\n",
      "Epoch 114, Loss: 0.012236700675107147\n",
      "Epoch 115, Loss: 0.012042718357406557\n",
      "Epoch 116, Loss: 0.01163072098585728\n",
      "Epoch 117, Loss: 0.011891735227484452\n",
      "Epoch 118, Loss: 0.011442119617162174\n",
      "Epoch 119, Loss: 0.009869287642160137\n",
      "Epoch 120, Loss: 0.013688647065703807\n",
      "Epoch 121, Loss: 0.01098065567202866\n",
      "Epoch 122, Loss: 0.01408134436994595\n",
      "Epoch 123, Loss: 0.012418571240758817\n",
      "Epoch 124, Loss: 0.012255240642269584\n",
      "Epoch 125, Loss: 0.010044252953345054\n",
      "Epoch 126, Loss: 0.01118808170461929\n",
      "Epoch 127, Loss: 0.011083597329592234\n",
      "Epoch 128, Loss: 0.012967686573239533\n",
      "Epoch 129, Loss: 0.011606423760225115\n",
      "Epoch 130, Loss: 0.011604060057403618\n",
      "Epoch 131, Loss: 0.01585575536898288\n",
      "Epoch 132, Loss: 0.01369372480487647\n",
      "Epoch 133, Loss: 0.010085507403250392\n",
      "Epoch 134, Loss: 0.008570423086271867\n",
      "Epoch 135, Loss: 0.012258820891331294\n",
      "Epoch 136, Loss: 0.013178130728192627\n",
      "Epoch 137, Loss: 0.010594443549818703\n",
      "Epoch 138, Loss: 0.012692401642102356\n",
      "Epoch 139, Loss: 0.009598927284394832\n",
      "Epoch 140, Loss: 0.012013326040591653\n",
      "Epoch 141, Loss: 0.012101894299695758\n",
      "Epoch 142, Loss: 0.009754962563563726\n",
      "Epoch 143, Loss: 0.011765296812978034\n",
      "Epoch 144, Loss: 0.013605932018866665\n",
      "Epoch 145, Loss: 0.012387741308071111\n",
      "Epoch 146, Loss: 0.01120409361197074\n",
      "Epoch 147, Loss: 0.012018578929679566\n",
      "Epoch 148, Loss: 0.016652499987302644\n",
      "Epoch 149, Loss: 0.01131806309996663\n",
      "Epoch 150, Loss: 0.010076645810745265\n",
      "Epoch 151, Loss: 0.011618476618375433\n",
      "Epoch 152, Loss: 0.01305690834200696\n",
      "Epoch 153, Loss: 0.010593124824625096\n",
      "Epoch 154, Loss: 0.010421043896042792\n",
      "Epoch 155, Loss: 0.010013702371476316\n",
      "Epoch 156, Loss: 0.009297057918861117\n",
      "Epoch 157, Loss: 0.011590424912834638\n",
      "Epoch 158, Loss: 0.011068718728462332\n",
      "Epoch 159, Loss: 0.008889701560531793\n",
      "Epoch 160, Loss: 0.013202405265091281\n",
      "Epoch 161, Loss: 0.01292789504728525\n",
      "Epoch 162, Loss: 0.012931244184361085\n",
      "Epoch 163, Loss: 0.012646805838142572\n",
      "Epoch 164, Loss: 0.008433898984405556\n",
      "Epoch 165, Loss: 0.008960117273473818\n",
      "Epoch 166, Loss: 0.009047376151245675\n",
      "Epoch 167, Loss: 0.011601729318499565\n",
      "Epoch 168, Loss: 0.0107130997409848\n",
      "Epoch 169, Loss: 0.012436111767065564\n",
      "Epoch 170, Loss: 0.01140275122720356\n",
      "Epoch 171, Loss: 0.009608939970157257\n",
      "Epoch 172, Loss: 0.01044897437340727\n",
      "Epoch 173, Loss: 0.012935466733516046\n",
      "Epoch 174, Loss: 0.011343771910392925\n",
      "Epoch 175, Loss: 0.010977497116025341\n",
      "Epoch 176, Loss: 0.013328057444213252\n",
      "Epoch 177, Loss: 0.014495263335687158\n",
      "Epoch 178, Loss: 0.01266979285221743\n",
      "Epoch 179, Loss: 0.011036370384604916\n",
      "Epoch 180, Loss: 0.009571317313729148\n",
      "Epoch 181, Loss: 0.012088968933216836\n",
      "Epoch 182, Loss: 0.012498125268489514\n",
      "Epoch 183, Loss: 0.01314158438655891\n",
      "Epoch 184, Loss: 0.013031694933919138\n",
      "Epoch 185, Loss: 0.012879442316357438\n",
      "Epoch 186, Loss: 0.010907491313685713\n",
      "Epoch 187, Loss: 0.009403578051684522\n",
      "Epoch 188, Loss: 0.011031017222098614\n",
      "Epoch 189, Loss: 0.009647218363457605\n",
      "Epoch 190, Loss: 0.013617824536356094\n",
      "Epoch 191, Loss: 0.011670013203432686\n",
      "Epoch 192, Loss: 0.011998420391280792\n",
      "Epoch 193, Loss: 0.013443528643907294\n",
      "Epoch 194, Loss: 0.008137740265259421\n",
      "Epoch 195, Loss: 0.011926286890612621\n",
      "Epoch 196, Loss: 0.008803881133453137\n",
      "Epoch 197, Loss: 0.008970749316320411\n",
      "Epoch 198, Loss: 0.012450171312880948\n",
      "Epoch 199, Loss: 0.009688506521725733\n",
      "Epoch 200, Loss: 0.011121505246448674\n",
      "Epoch 201, Loss: 0.010597639776901096\n",
      "Epoch 202, Loss: 0.01050502507285656\n",
      "Epoch 203, Loss: 0.01066646896805124\n",
      "Epoch 204, Loss: 0.010942380094140964\n",
      "Epoch 205, Loss: 0.012899252604448089\n",
      "Epoch 206, Loss: 0.00998564437338102\n",
      "Epoch 207, Loss: 0.013412253146893099\n",
      "Epoch 208, Loss: 0.010464632816269602\n",
      "Epoch 209, Loss: 0.011031807957854318\n",
      "Epoch 210, Loss: 0.012047663949871142\n",
      "Epoch 211, Loss: 0.014840773610079563\n",
      "Epoch 212, Loss: 0.01353648563503827\n",
      "Epoch 213, Loss: 0.010659937244398813\n",
      "Epoch 214, Loss: 0.008761526704275687\n",
      "Epoch 215, Loss: 0.011280044499711183\n",
      "Epoch 216, Loss: 0.012417899126088932\n",
      "Epoch 217, Loss: 0.011468283194852503\n",
      "Epoch 218, Loss: 0.013254169723950326\n",
      "Epoch 219, Loss: 0.013721569940619367\n",
      "Epoch 220, Loss: 0.011895798805390337\n",
      "Epoch 221, Loss: 0.012117106504248161\n",
      "Epoch 222, Loss: 0.011201644824866793\n",
      "Epoch 223, Loss: 0.013581069057660275\n",
      "Epoch 224, Loss: 0.010057822393719107\n",
      "Epoch 225, Loss: 0.014008145209548897\n",
      "Epoch 226, Loss: 0.01015643017538088\n",
      "Epoch 227, Loss: 0.010856007526040469\n",
      "Epoch 228, Loss: 0.009879228465969822\n",
      "Epoch 229, Loss: 0.00919701738626157\n",
      "Epoch 230, Loss: 0.01297192597198055\n",
      "Epoch 231, Loss: 0.012245927744324467\n",
      "Epoch 232, Loss: 0.009749277274271375\n",
      "Epoch 233, Loss: 0.010570593607170801\n",
      "Epoch 234, Loss: 0.010709553338146131\n",
      "Epoch 235, Loss: 0.011090363663817314\n",
      "Epoch 236, Loss: 0.01062324693990185\n",
      "Epoch 237, Loss: 0.012289969872462711\n",
      "Epoch 238, Loss: 0.0122150092471489\n",
      "Epoch 239, Loss: 0.011361550839960967\n",
      "Epoch 240, Loss: 0.014978694747888335\n",
      "Epoch 241, Loss: 0.010229727798631709\n",
      "Epoch 242, Loss: 0.009513379230874738\n",
      "Epoch 243, Loss: 0.009809806777507458\n",
      "Epoch 244, Loss: 0.014221192772598252\n",
      "Epoch 245, Loss: 0.01425265106311264\n",
      "Epoch 246, Loss: 0.01236248531342043\n",
      "Epoch 247, Loss: 0.008461471794075087\n",
      "Epoch 248, Loss: 0.01053910402850689\n",
      "Epoch 249, Loss: 0.013700443902052939\n",
      "Epoch 250, Loss: 0.011892600119800159\n",
      "Epoch 251, Loss: 0.015341454608316877\n",
      "Epoch 252, Loss: 0.010746334033013371\n",
      "Epoch 253, Loss: 0.012141480646708882\n",
      "Epoch 254, Loss: 0.010312027267278418\n",
      "Epoch 255, Loss: 0.011633109117514993\n",
      "Epoch 256, Loss: 0.008452594819429674\n",
      "Epoch 257, Loss: 0.011059975251555443\n",
      "Epoch 258, Loss: 0.012513060703273177\n",
      "Epoch 259, Loss: 0.00860543653221899\n",
      "Epoch 260, Loss: 0.010879155450598582\n",
      "Epoch 261, Loss: 0.012635339572290448\n",
      "Epoch 262, Loss: 0.01276959447913166\n",
      "Epoch 263, Loss: 0.011594581270688459\n",
      "Epoch 264, Loss: 0.009759575165318031\n",
      "Epoch 265, Loss: 0.008950261233717595\n",
      "Epoch 266, Loss: 0.011352339578718928\n",
      "Epoch 267, Loss: 0.011365829954708093\n",
      "Epoch 268, Loss: 0.011311272900600574\n",
      "Epoch 269, Loss: 0.0111862959854893\n",
      "Epoch 270, Loss: 0.013195213959797433\n",
      "Epoch 271, Loss: 0.012131615682799173\n",
      "Epoch 272, Loss: 0.011087023389623746\n",
      "Epoch 273, Loss: 0.008505324119285337\n",
      "Epoch 274, Loss: 0.013802464107835763\n",
      "Epoch 275, Loss: 0.012513412818263629\n",
      "Epoch 276, Loss: 0.00824619090264222\n",
      "Epoch 277, Loss: 0.012645991485394342\n",
      "Epoch 278, Loss: 0.01018970065112961\n",
      "Epoch 279, Loss: 0.009799645931803082\n",
      "Epoch 280, Loss: 0.01052907036654161\n",
      "Epoch 281, Loss: 0.009433857704463759\n",
      "Epoch 282, Loss: 0.00854362109308376\n",
      "Epoch 283, Loss: 0.009761048713698983\n",
      "Epoch 284, Loss: 0.010627716485606996\n",
      "Epoch 285, Loss: 0.008691995588474367\n",
      "Epoch 286, Loss: 0.0093455976855598\n",
      "Epoch 287, Loss: 0.011562010506168008\n",
      "Epoch 288, Loss: 0.011020302613216796\n",
      "Epoch 289, Loss: 0.010481626077212002\n",
      "Epoch 290, Loss: 0.013317896782322541\n",
      "Epoch 291, Loss: 0.012517686957787526\n",
      "Epoch 292, Loss: 0.008957403010419128\n",
      "Epoch 293, Loss: 0.011251779450838896\n",
      "Epoch 294, Loss: 0.007928056388137568\n",
      "Epoch 295, Loss: 0.011871323951421991\n",
      "Epoch 296, Loss: 0.010623140790573271\n",
      "Epoch 297, Loss: 0.011185772965147504\n",
      "Epoch 298, Loss: 0.011462473776191473\n",
      "Epoch 299, Loss: 0.010366233249538039\n",
      "Epoch 300, Loss: 0.014185360479658763\n",
      "Epoch 301, Loss: 0.011163124676769305\n",
      "Epoch 302, Loss: 0.01058382286665667\n",
      "Epoch 303, Loss: 0.008374593425909743\n",
      "Epoch 304, Loss: 0.013017409242150424\n",
      "Epoch 305, Loss: 0.010415545137795178\n",
      "Epoch 306, Loss: 0.010046878572259294\n",
      "Epoch 307, Loss: 0.011879800435915393\n",
      "Epoch 308, Loss: 0.008195350620601522\n",
      "Epoch 309, Loss: 0.009631908670263855\n",
      "Epoch 310, Loss: 0.010642766710493322\n",
      "Epoch 311, Loss: 0.009786262660973558\n",
      "Epoch 312, Loss: 0.00911164446733892\n",
      "Epoch 313, Loss: 0.010594817356353528\n",
      "Epoch 314, Loss: 0.012718726727670352\n",
      "Epoch 315, Loss: 0.011474512756410005\n",
      "Epoch 316, Loss: 0.011620791273583708\n",
      "Epoch 317, Loss: 0.013631369587720224\n",
      "Epoch 318, Loss: 0.011812930257292464\n",
      "Epoch 319, Loss: 0.009379994038394407\n",
      "Epoch 320, Loss: 0.013144101592172918\n",
      "Epoch 321, Loss: 0.010280890718077947\n",
      "Epoch 322, Loss: 0.010433513735494527\n",
      "Epoch 323, Loss: 0.010176752130255887\n",
      "Epoch 324, Loss: 0.009625128841052126\n",
      "Epoch 325, Loss: 0.009875800806759415\n",
      "Epoch 326, Loss: 0.010414671695080438\n",
      "Epoch 327, Loss: 0.00781290103699767\n",
      "Epoch 328, Loss: 0.011487214770903321\n",
      "Epoch 329, Loss: 0.012351772965446702\n",
      "Epoch 330, Loss: 0.012096833125197966\n",
      "Epoch 331, Loss: 0.008357041117147003\n",
      "Epoch 332, Loss: 0.009738307577974507\n",
      "Epoch 333, Loss: 0.011452560156804362\n",
      "Epoch 334, Loss: 0.009741737752368576\n",
      "Epoch 335, Loss: 0.007566800763781525\n",
      "Epoch 336, Loss: 0.010988054821561826\n",
      "Epoch 337, Loss: 0.009453803810940468\n",
      "Epoch 338, Loss: 0.01022587732203599\n",
      "Epoch 339, Loss: 0.009551456709710979\n",
      "Epoch 340, Loss: 0.010445006112077911\n",
      "Epoch 341, Loss: 0.009713708165738928\n",
      "Epoch 342, Loss: 0.009742371964660523\n",
      "Epoch 343, Loss: 0.013228293918808432\n",
      "Epoch 344, Loss: 0.008167603145059394\n",
      "Epoch 345, Loss: 0.007513187454988886\n",
      "Epoch 346, Loss: 0.009815018366728174\n",
      "Epoch 347, Loss: 0.010723475676250496\n",
      "Epoch 348, Loss: 0.010360447696017983\n",
      "Epoch 349, Loss: 0.010722656930355649\n",
      "Epoch 350, Loss: 0.00841224852851347\n",
      "Epoch 351, Loss: 0.011000119805286982\n",
      "Epoch 352, Loss: 0.007817174755281917\n",
      "Epoch 353, Loss: 0.008406202063748711\n",
      "Epoch 354, Loss: 0.01294659450037503\n",
      "Epoch 355, Loss: 0.008454141890213481\n",
      "Epoch 356, Loss: 0.009631929323261016\n",
      "Epoch 357, Loss: 0.00973811676713491\n",
      "Epoch 358, Loss: 0.008247761294784906\n",
      "Epoch 359, Loss: 0.009646841625094806\n",
      "Epoch 360, Loss: 0.009262767687783037\n",
      "Epoch 361, Loss: 0.009847088233647426\n",
      "Epoch 362, Loss: 0.006808121788229695\n",
      "Epoch 363, Loss: 0.009298256500379035\n",
      "Epoch 364, Loss: 0.008897532523597443\n",
      "Epoch 365, Loss: 0.009463788984392426\n",
      "Epoch 366, Loss: 0.009663063469097802\n",
      "Epoch 367, Loss: 0.009553794099606182\n",
      "Epoch 368, Loss: 0.007125953877173168\n",
      "Epoch 369, Loss: 0.008884195759157209\n",
      "Epoch 370, Loss: 0.010427155777027733\n",
      "Epoch 371, Loss: 0.008181828585771942\n",
      "Epoch 372, Loss: 0.008940108457433158\n",
      "Epoch 373, Loss: 0.0096757347622004\n",
      "Epoch 374, Loss: 0.009933452912312197\n",
      "Epoch 375, Loss: 0.010230804137934587\n",
      "Epoch 376, Loss: 0.00979332750246517\n",
      "Epoch 377, Loss: 0.01559094016709806\n",
      "Epoch 378, Loss: 0.011097137206601665\n",
      "Epoch 379, Loss: 0.006732717021613529\n",
      "Epoch 380, Loss: 0.009098363084424483\n",
      "Epoch 381, Loss: 0.011279356494349869\n",
      "Epoch 382, Loss: 0.009802582263211278\n",
      "Epoch 383, Loss: 0.009338887861782783\n",
      "Epoch 384, Loss: 0.008254200333459792\n",
      "Epoch 385, Loss: 0.009287441697731418\n",
      "Epoch 386, Loss: 0.010951452217628494\n",
      "Epoch 387, Loss: 0.009581425506008887\n",
      "Epoch 388, Loss: 0.010757224980472145\n",
      "Epoch 389, Loss: 0.011127793129018852\n",
      "Epoch 390, Loss: 0.007325015868685257\n",
      "Epoch 391, Loss: 0.008264960598592696\n",
      "Epoch 392, Loss: 0.007194421232653488\n",
      "Epoch 393, Loss: 0.012547103366446927\n",
      "Epoch 394, Loss: 0.00969053715418436\n",
      "Epoch 395, Loss: 0.007211557025776098\n",
      "Epoch 396, Loss: 0.011351212840772382\n",
      "Epoch 397, Loss: 0.008530863842583801\n",
      "Epoch 398, Loss: 0.010058528994896301\n",
      "Epoch 399, Loss: 0.008208380693471745\n",
      "Epoch 400, Loss: 0.009401397117854733\n",
      "Epoch 401, Loss: 0.010999953539698924\n",
      "Epoch 402, Loss: 0.012651029986476428\n",
      "Epoch 403, Loss: 0.008911596451836982\n",
      "Epoch 404, Loss: 0.0082445544049781\n",
      "Epoch 405, Loss: 0.010411960204548546\n",
      "Epoch 406, Loss: 0.01044987654007089\n",
      "Epoch 407, Loss: 0.010073576817950724\n",
      "Epoch 408, Loss: 0.011968517669868705\n",
      "Epoch 409, Loss: 0.008020003185313391\n",
      "Epoch 410, Loss: 0.011802592137093214\n",
      "Epoch 411, Loss: 0.010476860897860638\n",
      "Epoch 412, Loss: 0.011786189267264777\n",
      "Epoch 413, Loss: 0.006118405368301626\n",
      "Epoch 414, Loss: 0.007527347274222656\n",
      "Epoch 415, Loss: 0.01024602652882765\n",
      "Epoch 416, Loss: 0.00898569448556947\n",
      "Epoch 417, Loss: 0.009532887926692829\n",
      "Epoch 418, Loss: 0.008071839205896188\n",
      "Epoch 419, Loss: 0.007535233088818036\n",
      "Epoch 420, Loss: 0.009716650590570154\n",
      "Epoch 421, Loss: 0.010743716596870831\n",
      "Epoch 422, Loss: 0.009912101582525984\n",
      "Epoch 423, Loss: 0.00894309517829434\n",
      "Epoch 424, Loss: 0.012242635693646184\n",
      "Epoch 425, Loss: 0.008894116440052656\n",
      "Epoch 426, Loss: 0.011130982090866095\n",
      "Epoch 427, Loss: 0.009995647133205478\n",
      "Epoch 428, Loss: 0.00975675981087414\n",
      "Epoch 429, Loss: 0.010884467892250732\n",
      "Epoch 430, Loss: 0.009729763676755522\n",
      "Epoch 431, Loss: 0.00946022068321901\n",
      "Epoch 432, Loss: 0.0078005152693214385\n",
      "Epoch 433, Loss: 0.011456998193783588\n",
      "Epoch 434, Loss: 0.008262756133550092\n",
      "Epoch 435, Loss: 0.00949377382762338\n",
      "Epoch 436, Loss: 0.009047559315436765\n",
      "Epoch 437, Loss: 0.01251846755928311\n",
      "Epoch 438, Loss: 0.012699905093636756\n",
      "Epoch 439, Loss: 0.010667134986234535\n",
      "Epoch 440, Loss: 0.008337134306968533\n",
      "Epoch 441, Loss: 0.010200844970409219\n",
      "Epoch 442, Loss: 0.010019371834412021\n",
      "Epoch 443, Loss: 0.011706637993675509\n",
      "Epoch 444, Loss: 0.009067180401057397\n",
      "Epoch 445, Loss: 0.010872816636325106\n",
      "Epoch 446, Loss: 0.010465954286413953\n",
      "Epoch 447, Loss: 0.013963144088752176\n",
      "Epoch 448, Loss: 0.010908893664906683\n",
      "Epoch 449, Loss: 0.008592098093542614\n",
      "Epoch 450, Loss: 0.010072269544348512\n",
      "Epoch 451, Loss: 0.009097359259612858\n",
      "Epoch 452, Loss: 0.00840361306970743\n",
      "Epoch 453, Loss: 0.008831271460573924\n",
      "Epoch 454, Loss: 0.009917246943682824\n",
      "Epoch 455, Loss: 0.01200103092215661\n",
      "Epoch 456, Loss: 0.010640044957031742\n",
      "Epoch 457, Loss: 0.00999829942348266\n",
      "Epoch 458, Loss: 0.012756302462596642\n",
      "Epoch 459, Loss: 0.008102768860561283\n",
      "Epoch 460, Loss: 0.009545088770162118\n",
      "Epoch 461, Loss: 0.007925361652221335\n",
      "Epoch 462, Loss: 0.008380685283459331\n",
      "Epoch 463, Loss: 0.007439263462162528\n",
      "Epoch 464, Loss: 0.011525851184208142\n",
      "Epoch 465, Loss: 0.012567769604318432\n",
      "Epoch 466, Loss: 0.010041917410777196\n",
      "Epoch 467, Loss: 0.010786954448313305\n",
      "Epoch 468, Loss: 0.007407101765765171\n",
      "Epoch 469, Loss: 0.010040974720859117\n",
      "Epoch 470, Loss: 0.007677973661645267\n",
      "Epoch 471, Loss: 0.008582272318706504\n",
      "Epoch 472, Loss: 0.010623254822444562\n",
      "Epoch 473, Loss: 0.010783356856432204\n",
      "Epoch 474, Loss: 0.010376872762878376\n",
      "Epoch 475, Loss: 0.007807713571788841\n",
      "Epoch 476, Loss: 0.010205359823119483\n",
      "Epoch 477, Loss: 0.01052874258082164\n",
      "Epoch 478, Loss: 0.012744732963926109\n",
      "Epoch 479, Loss: 0.009557167732916576\n",
      "Epoch 480, Loss: 0.010765154501361968\n",
      "Epoch 481, Loss: 0.010549833106587789\n",
      "Epoch 482, Loss: 0.008495748655772522\n",
      "Epoch 483, Loss: 0.011022883142638756\n",
      "Epoch 484, Loss: 0.00863266137342802\n",
      "Epoch 485, Loss: 0.007742993604073203\n",
      "Epoch 486, Loss: 0.011292719962320438\n",
      "Epoch 487, Loss: 0.007801421571792544\n",
      "Epoch 488, Loss: 0.010904157222524335\n",
      "Epoch 489, Loss: 0.008142662959136559\n",
      "Epoch 490, Loss: 0.009481785439052865\n",
      "Epoch 491, Loss: 0.00921822890430983\n",
      "Epoch 492, Loss: 0.009954418769522914\n",
      "Epoch 493, Loss: 0.0070820463925453\n",
      "Epoch 494, Loss: 0.008135837616129337\n",
      "Epoch 495, Loss: 0.008061852686009124\n",
      "Epoch 496, Loss: 0.012898597366936309\n",
      "Epoch 497, Loss: 0.007345225735518493\n",
      "Epoch 498, Loss: 0.011421520465151653\n",
      "Epoch 499, Loss: 0.009670712957526311\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import Repository, create_repo\n",
    "from diffusers import DDPMPipeline\n",
    "from torch.nn.parallel import DataParallel\n",
    "import os\n",
    "import torch\n",
    "import torch\n",
    "\n",
    "#import torch\n",
    "#import torch.nn.parallel\n",
    "# 假设其他必要的库和函数已经导入\n",
    "\n",
    "def train():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    model.train()\n",
    "    # 创建一个目录来保存训练结果\n",
    "    save_dir = './savecatr500'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 创建一个文件用于记录loss\n",
    "    loss_file = os.path.join(save_dir, 'losses.txt')\n",
    "    with open(loss_file, 'w') as f:\n",
    "        f.write('Epoch, Loss\\n')  # 写入标题行\n",
    "\n",
    "    loss_sum = 0\n",
    "    for epoch in range(500):\n",
    "        for i, data in enumerate(loader):\n",
    "            loss = get_loss(data['Images'].to(device))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler_lr.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_sum += loss.item()  # 累积loss\n",
    "\n",
    "        # 计算平均loss\n",
    "        average_loss = loss_sum / len(loader)\n",
    "        loss_sum = 0  # 重置累积loss\n",
    "\n",
    "        # 打印当前epoch的loss\n",
    "        print(f'Epoch {epoch}, Loss: {average_loss}')\n",
    "\n",
    "        # 将当前epoch的loss写入文件\n",
    "        with open(loss_file, 'a') as f:\n",
    "            f.write(f'{epoch}, {average_loss:.6f}\\n')  # 写入当前epoch和loss\n",
    "\n",
    "    # 保存模型\n",
    "    DDPMPipeline(unet=model, scheduler=scheduler).save_pretrained(save_dir)\n",
    "\n",
    "\n",
    "\n",
    "# 调用训练函数\n",
    "train()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
